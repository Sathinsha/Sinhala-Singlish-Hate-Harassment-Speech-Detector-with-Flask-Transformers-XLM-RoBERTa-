# ğŸš« Hate & Harassment Speech Detection (Sinhala/Singlish)

Transformers-powered, realâ€‘time hate & harassment speech detector with a modern web UI, tokenâ€‘level (wordâ€‘level) analysis, feedback loop, and oneâ€‘click incremental fineâ€‘tuning

<p align="center">
  <img src="https://img.shields.io/badge/Framework-Transformers-%23555555?logo=huggingface"/>
  <img src="https://img.shields.io/badge/Backend-Flask-%23000000?logo=flask"/>
  <img src="https://img.shields.io/badge/Model-XLM--RoBERTa-%233b82f6"/>
  <img src="https://img.shields.io/badge/Tokenization-Word--level-%2310b981"/>
  <img src="https://img.shields.io/badge/Status-Active-%2310b981"/>
</p>

---

## âœ¨ Highlights
- ğŸ§  XLMâ€‘RoBERTa tokenâ€‘classification (2 labels: Neutral/Hate)
- ğŸ”¤ Wordâ€‘level tokenization (no UNK for Sinhala): robust to Sinhala/Singlish
- âš¡ Realâ€‘time web UI (Flask) with modern, polished interface
- ğŸ‘€ Autoâ€‘hide for HATE outputs + View/Hide toggle
- ğŸ§© Tokenâ€‘level (wordâ€‘level) explanations & detected hate words
- ğŸ“ Feedback loop (flag false positives/false negatives)
- ğŸ” Incremental fineâ€‘tuning on feedback CSV without overwriting base model

---

## ğŸ–¥ï¸ Demo (Local)
- Start the server:
  ```bash
  python app.py
  ```
- Open: `http://localhost:5000`
- Try examples (Sinhala/Singlish) and check token highlights + confidence.

> Tip: HATE detections autoâ€‘hide. Click â€œViewâ€ to reveal wordâ€‘level details.

---

## ğŸ—ï¸ Architecture
- `XLMâ€‘RoBERTa` for token classification (Hugging Face Transformers)
- Wordâ€‘level tokenization, labels aligned to words
- Flask backend with a modern responsive UI
- Feedback collector â†’ CSV â†’ fineâ€‘tune script â†’ hotâ€‘swap latest snapshot

```
[UI] â†’ /detect â†’ [Inferencer (XLMâ€‘R)] â†’ word predictions + confidence
  â””â†’ /feedback/* â†’ user_flags.csv â†’ retrain_on_feedback.py â†’ new snapshot
         â””â†’ /admin/retrain hot-swaps model (keeps base intact)
```

---

## ğŸ“¦ Project Structure
```
PRo/
â”œâ”€â”€ app.py                         # Flask app (lazy model init, endpoints)
â”œâ”€â”€ inference_xlm_roberta.py       # Inference (word-level, confidence logic)
â”œâ”€â”€ train_xlm_roberta.py           # Full training (dataset)
â”œâ”€â”€ data_preprocessing_xlm_roberta.py
â”œâ”€â”€ retrain_on_feedback.py         # Incremental fine-tune on feedback CSV
â”œâ”€â”€ config_xlm_roberta.py          # Config + paths (model, feedback)
â”œâ”€â”€ templates/index.html           # Modern UI (sidebar, colorful sections)
â”œâ”€â”€ outputs_xlm_roberta/           # Model/results outputs
â”œâ”€â”€ feedback/user_flags.csv        # Collected feedback (appends)
â””â”€â”€ datsets/                       # SOLD dataset & only_hate.csv
```

---

## ğŸš€ Quick Start

### 1) Environment
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

pip install -r requirements.txt
```

### 2) Test inference locally
```bash
python inference_xlm_roberta.py
```

### 3) Run the Web App
```bash
python app.py
```
Open `http://localhost:5000`.

---

## ğŸ§ª Training (Full Dataset)
Run a full train (GPU recommended) or use Google Colab:
```bash
python train_xlm_roberta.py
```
This saves the base model to `outputs_xlm_roberta/model`.

> Colab tip: upload `datsets/`, run `train_xlm_roberta.py`, then download the `outputs_xlm_roberta/model` back into this project.

---

## ğŸ” Inference Details
- Wordâ€‘level tokenization (space split), then convert each word to subtokens; we aggregate confidence per word (max p(hate) across subtokens). 
- Sentence label: HATE if any word is predicted HATE, or if the dictionary triggers.
- Confidence: mean p(hate) over predicted hate words; if dictionary triggers alone â†’ fallback confidence (e.g., 0.8).

---

## ğŸ“ Feedback & Continuous Learning
The UI provides feedback buttons:
- HATE â†’ â€œDo you think this is NOT HATE?â€
- NOT HATE â†’ â€œDo you think this is HATE?â€

Feedback is stored at:
```
feedback/user_flags.csv
# columns:
# timestamp,input_text,model_label,user_thinks_label,user_words,client_ip,user_agent
```

### Fineâ€‘tune on feedback only (keeps base model)
```bash
python retrain_on_feedback.py
```
- Saves to `outputs_xlm_roberta/fine_tuned/<timestamp>`
- Does NOT overwrite `outputs_xlm_roberta/model`

### Hotâ€‘swap to latest fineâ€‘tuned model (from the app)
```bash
curl -X POST http://localhost:5000/admin/retrain \
  -H "Content-Type: application/json" \
  -d '{"epochs":2, "lr":2e-5, "batch_size":8}'
```
Returns the new snapshot path and reloads the inâ€‘memory model.

> Best practice: freeze most layers or use small epochs to avoid drift. You can also mix in a tiny replay subset to mitigate forgetting.

---

## ğŸ”§ Configuration
See `config_xlm_roberta.py`:
- Model & paths (`MODEL_SAVE_PATH`, `RESULTS_SAVE_PATH`)
- Feedback CSV location (`FEEDBACK_CSV`)
- Fineâ€‘tune output base (`RETRAIN_OUTPUT_BASE`)
- Tokenization type (`TOKENIZATION_TYPE = "word_level_by_spaces"`)
- Training hyperparameters

---

## ğŸŒ REST API
- POST `/detect`
  ```json
  { "text": "#### à¶¸à·à¶©à¶ºà·" }
  ```
- POST `/feedback/hate_missed` (NOT HATE â†’ actually HATE)
- POST `/feedback/false_positive` (HATE â†’ actually NOT HATE)
- POST `/admin/retrain` (fineâ€‘tune on feedback + swap snapshot)
- GET `/stats`, GET `/health`

---

## ğŸ§¾ Dataset
- SOLD train/test JSON (tokens + labels per word)
- `only_hate.csv` dictionary assists ruleâ€‘based fallback

> Labels: `0 = Neutral`, `1 = Hate`. The UI highlights hate words and confidence.

---

## ğŸ–¼ï¸ UI Preview
- Centered header + status badge
- Left sidebar with smoothâ€‘scroll + active section highlighting
- Colorful section panels (Input / System Overview / Results / Developers)
- Autoâ€‘hide HATE details with View/Hide control
- Inline feedback form (no popups)

---

## ğŸ‘¥ Developers

```
22ug-0722, 22ug1-0634, 22ug1-0360, 22ug1-0763,
22ug1-0754, 22ug1-0802, 21ug1128, 21ug1014, 21ug0990, 21ug1089
```

---

## ğŸ“œ License
This repository is for educational and research purposes. Ensure compliance with local regulations regarding hate speech detection and data usage.

---

## ğŸ™Œ Acknowledgments
- SOLD Dataset
- Hugging Face Transformers
- XLMâ€‘RoBERTa

> PRs and issues welcome â€“ feel free to open a discussion for feature requests or improvements!

