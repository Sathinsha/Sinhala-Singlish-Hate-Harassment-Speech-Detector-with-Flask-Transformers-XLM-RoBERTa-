"""
Test script for XLM-RoBERTa hate speech detection setup
Verifies tokenization, data preprocessing, and model loading
"""

from transformers import AutoTokenizer
from data_preprocessing_xlm_roberta import load_sold_dataset, create_xlm_roberta_dataset
import config_xlm_roberta as config

def test_xlm_roberta_setup():
    """Test the complete XLM-RoBERTa setup"""
    
    print("üß™ Testing XLM-RoBERTa Setup for Sinhala Hate Speech Detection")
    print("=" * 60)
    
    # Test 1: Tokenizer
    print("\n1Ô∏è‚É£ Testing XLM-RoBERTa Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)
        print(f"‚úÖ Tokenizer loaded: {config.MODEL_NAME}")
        
        # Test Sinhala text tokenization
        test_texts = [
            "‡∂∏‡∑ù‡∂©‡∂∫‡∑è ‡∂ë‡∂≠‡∂± ‡∂∫‡∂±‡∑ä‡∂±",  # "Go there, idiot"
            "‡∑Ñ‡∑î‡∂≠‡∑ä‡∂≠‡∑ú ‡∂ë‡∂≠‡∂± ‡∂∫‡∂±‡∑ä‡∂±",   # "Go there, fool"
            "‡∂∏‡∂∏ ‡∂î‡∂∂‡∂ß ‡∂ã‡∂Ø‡∑Ä‡∑ä ‡∂ö‡∂ª‡∂±‡∑ä‡∂±‡∂∏‡∑ä",  # "I will help you"
        ]
        
        for text in test_texts:
            tokens = tokenizer.tokenize(text)
            unk_count = tokens.count('[UNK]') if '[UNK]' in tokens else 0
            print(f"   '{text}' -> {tokens} (UNK: {unk_count})")
            
        if unk_count == 0:
            print("‚úÖ No UNK tokens - Sinhala text properly tokenized!")
        else:
            print("‚ö†Ô∏è  UNK tokens found - tokenization may have issues")
            
    except Exception as e:
        print(f"‚ùå Tokenizer error: {e}")
        return False
    
    # Test 2: Data Loading
    print("\n2Ô∏è‚É£ Testing Data Loading...")
    try:
        train_tokens, train_labels = load_sold_dataset(config.TRAIN_FILE)
        print(f"‚úÖ Training data loaded: {len(train_tokens)} samples")
        
        # Show sample data
        print(f"   Sample tokens: {train_tokens[0][:5]}...")
        print(f"   Sample labels: {train_labels[0][:5]}...")
        
    except Exception as e:
        print(f"‚ùå Data loading error: {e}")
        return False
    
    # Test 3: Dataset Creation
    print("\n3Ô∏è‚É£ Testing Dataset Creation...")
    try:
        train_texts, train_processed_labels = create_xlm_roberta_dataset(
            train_tokens[:10], train_labels[:10], tokenizer, config.MAX_LENGTH
        )
        print(f"‚úÖ Dataset created: {len(train_texts)} processed samples")
        print(f"   Max length: {config.MAX_LENGTH}")
        print(f"   Sample processed labels: {train_processed_labels[0][:10]}...")
        
    except Exception as e:
        print(f"‚ùå Dataset creation error: {e}")
        return False
    
    # Test 4: Configuration
    print("\n4Ô∏è‚É£ Testing Configuration...")
    print(f"   Model: {config.MODEL_NAME}")
    print(f"   Max length: {config.MAX_LENGTH}")
    print(f"   Batch size: {config.BATCH_SIZE}")
    print(f"   Learning rate: {config.LEARNING_RATE}")
    print(f"   Epochs: {config.NUM_EPOCHS}")
    print(f"   Output directory: {config.OUTPUT_DIR}")
    
    # Test 5: Output Directories
    print("\n5Ô∏è‚É£ Testing Output Directories...")
    import os
    if os.path.exists(config.OUTPUT_DIR):
        print(f"‚úÖ Output directory exists: {config.OUTPUT_DIR}")
    else:
        print(f"üìÅ Output directory will be created: {config.OUTPUT_DIR}")
    
    print("\nüéØ Setup Summary:")
    print("   ‚úÖ XLM-RoBERTa tokenizer works with Sinhala text")
    print("   ‚úÖ Data loading and preprocessing functional")
    print("   ‚úÖ Configuration properly set")
    print("   ‚úÖ Ready for Google Colab training!")
    
    return True

if __name__ == "__main__":
    test_xlm_roberta_setup()
